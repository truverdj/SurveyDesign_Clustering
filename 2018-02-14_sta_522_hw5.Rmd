---
title: "STA 522 HW5 (Clustering)"
author: "Daniel Truver"
date: "2/14/2018"
header-includes:
  - \usepackage{amsmath}
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### (1) Lohr, Chapter 5 Problem 1

No, these estimates are bull. First of all, there are non-respondents, which I'm told is a problem. The estimate does not account for the non-response in any way that I can see. It also does not acount for the cluster sample design. This estimator treats each individual as though they were the draw of a simple random sample rather than elements of clusters. They do not even take the number of clusters into account. 

#### (2) Lohr, Chapter 5 Problem 3

##### (a) Sampling Units 

The psus are the wetlands themselves. Within the set of all possible areas one could measure the desired quantities, the wetlands are the clusters. The ssus are the sites within each of the wetlands. From the paper, it appears they were 1-$m^2$ areas determined by tossing a square of PVC pipe into the air and taking measurements where it landed. Those crazy ecologists at it again. 

From the paper, it seems that the wetlands were selected were a judgement sample. I would treat the designations of "small town" and "suburban" as strata and estimate average pH using the Horvitz-Thompson estimator and treating the two suburban wetlands as clusters within the population of suburban wetlands. 

##### (b) Independence of Sites

The sites (ssus) should not be considered independent. From practice and intuition, samples within the same cluster tend to be correlated with each other. That is, we have broken the iid assumption. 

#### (3) Lohr, Chapter 5 Problem 4

##### (a) Sampling Discussion

The set of all social and behavioral science articles published in 1988 is clustered by journal. The journals are then the psus. This is a single stage cluster sample since every empirical article in the journal is represented in the survey.  

##### (b) Articles with Non-probability Sampling

We will estimate the total number of articles in the 1285 journals as well as the total number of that use nonprobability sampling. 

```{r setupTheData, message=FALSE, warning=FALSE}
journals = read.csv("journals.csv") 
library(survey)
library(dplyr)
N.cluster = 1285
n.cluster = nrow(journals)
journals = journals %>%
  mutate(wts.cluster = N.cluster/n.cluster) %>%
  mutate(fpc.cluster = N.cluster) 
svy.jour = svydesign(~X, weights = journals$wts.cluster, fpc = journals$fpc.cluster,
                     data = journals)
total.articles = svytotal(~numemp, svy.jour)
confi.articles = confint(total.articles)
total.nonprob = svytotal(~nonprob, svy.jour)
confi.nonprob = confint(total.nonprob)
prop_nonprob = total.nonprob[1]/total.articles[1]
# I can't find a way to calculate this in the survey package
prop_SE = sqrt((1-n.cluster/N.cluster)*1/(n.cluster*mean(journals$numemp)^2)*
                 sum((journals$nonprob-prop_nonprob*journals$numemp)^2)/(n.cluster-1))
```

```{r displayRes3b, echo=FALSE}
knitr::kable(data.frame(Proportion = prop_nonprob, SE = prop_SE),
             caption = "Proportion of Articles with Non-probability Sample",
             row.names = FALSE, digits = 3)
```

##### (c) Comments on Admissability

The common use of a technique does not prove its merit in court. We see this problem with eye witness testimony and bite mark analysis. Both have been used in court with disturbing frequency. Disturbing, since there is a non-trivial volume of research casting doubt on these practices. The technique and reason for non-probability sampling should be of grave concern. Was the sample due to accident, perhaps the only avaible data? Was it the judgement call of the researcher? The accuracy of the survey results is immediately questionable on the statistical level. 

On the human level, it is unlikely that a jury will have knowledge of survery design. A well-presented article from "recognized scholarly and practitioner experts" will sound convincing to the unitiated. A rebuttal of such evidence would require a lecture on surveys, which, as we all know, is a guaranteed way  to win over a court room. Only Professor Reiter could pull it off because his lectures are the best and he deserves a raise. Really tremendous guy. If I ruled the world, (a) there'd be no crime under my benevolent yet absolute leadership, and (b) all survey evidence submitted to the courts would undergo review by experts in the field before it appeared in front of a judge, jury, or executioner.

#### (4) Lohr, Chapter 5 Problem 7

First opinions, don't name a candy Green Globules. 

```{r settingUpTheData}
N.cities = 45
n.cities = 6
M_i = c(52, 19,37,39,8,14)
y = list()
y[[1]] = c(146, 180,251,152, 72,181,171,361,73,186); m_1 = length(y[[1]])
y[[2]] = c(99,101,52,121) ; m_2 = length(y[[2]])
y[[3]] = c(199,179,98,63,126,87,62); m_3 = length(y[[3]])
y[[4]] = c(226,129,57,46,86,43,85,165); m_4 = length(y[[4]])
y[[5]] = c(12,23); m_5 = length(y[[5]])
y[[6]] = c(87,43,59); m_6 = length(y[[6]])
store.df = data.frame(cases = unlist(y)) %>%
  mutate(city = c(rep(1, m_1), rep(2, m_2), rep(3, m_3), 
                  rep(4, m_4), rep(5, m_5), rep(6, m_6))) %>%
  mutate(numMarkets = c(rep(M_i[1], m_1), rep(M_i[2], m_2), rep(M_i[3], m_3),
                        rep(M_i[4], m_4), rep(M_i[5], m_5), rep(M_i[6], m_6))) %>%
  mutate(numSample = c(rep(m_1,m_1), rep(m_2, m_2), rep(m_3, m_3),
                       rep(m_4, m_4), rep(m_5, m_5), rep(m_6, m_6))) %>%
  mutate(N.clusters = N.cities) %>%
  mutate(n.clusters = n.cities) %>%
  mutate(marketID = 1:nrow(.)) %>%
  mutate(wts = N.clusters/n.clusters * numMarkets/numSample)
svy.cases = svydesign(~city+marketID, weights = store.df$wts, 
                      fpc = ~store.df$N.clusters+store.df$numMarkets,
                      data = store.df)
total.cases = data.frame(svytotal(~cases, svy.cases))
mean.cases = data.frame(svymean(~cases, svy.cases))
knitr::kable(cbind(total.cases, mean.cases), 
             col.names = c("Total", "SE(total)", "Mean", "SE(mean)"),
             caption = "Cases of Green Globules Sold in Upstate New York Supermarkets",
             digits = 2)
```


#### (5) Simulation Study

##### (a) Creating the Population 

```{r generatePopulation}
N.clusters = N = 100
Mi = c(rep(100,40), rep(300,40), rep(500, 20))
M_0 = sum(Mi)
run.cores = parallel::detectCores()/2
value = parallel::mclapply(Mi,
                           function(M){
                             if (M == 100){
                               rnorm(M, 40, 5)
                             } else if (M == 300){
                               rnorm(M, 10, 2)
                             } else if (M == 500){
                               rnorm(M, 100, 20)
                             }
                           },
                           mc.cores = run.cores)
```

##### (b) Running the Simulation

```{r runSimulation}
set.seed(2)
T = 1000
res = data.frame(numCases = rep(NA, T), mean = rep(NA, T), var = rep(NA, T),
                 HTmean = rep(NA, T), HTvar = rep(NA, T))
for (t in 1:T){
  n.clusters = n = 10
  svy = sample(1:N, n)
  M_i = Mi[svy] 
  res[t, "numCases"] = m = sum(M_i)
  y = value[svy]
  res[t, "mean"] = sampleMean = mean(unlist(y))
  s.sqr = sum((unlist(y) - sampleMean)^2)/(m-1)
  res[t, "var"] = (1-m/M_0) * s.sqr/m 
  HTtotal = (N/n * sum(unlist(lapply(y, sum))))
  res[t, "HTmean"] = HTtotal/M_0
  s.sqr_t = (1/(n-1)) * sum((unlist(lapply(y, sum)) - HTtotal/N)^2)
  res[t, "HTvar"] = (1/M_0^2) * N^2*(1-n/N)*s.sqr_t/n
}
res.avg = rbind.data.frame(apply(res, 2, mean)[2:5])
res.var = c(var(res$mean), var(res$HTmean))
```

```{r knitRes5b}
knitr::kable(cbind(res.avg, mean(unlist(value)), var(unlist(value))), 
             col.names = c("Avg Sample Mean", "Avg Sample Variance", "Avg HT Mean", 
                           "Avg HT Variance", "True Population Mean", "True Population Variance"),
             caption = "Population Estimates",
             digits = 2)
knitr::kable(data.frame(res.var[1], res.var[2]),
             col.names = c("Variance of Sample Mean", "Variance of HT Mean"),
             caption = "Variance of Estimates",
             digits = 2)
```

##### (c) Empirical Bias Results

The results above suggest that $\tilde{y}$ is biased and underestimates the population mean on average, but only slightly and not much worse than the HT estimator. 

##### (d) Bias in Variance

The results above indicated that $\hat{var}(\tilde{y})$ is a disgusting underestimate for the between cluster variance. It is almost suspiciously low, but I cannot find an error in the code.

##### (e) HT Bias

The theory lines up with results for the HT estimate of the mean. The variance, on the other hand, is significantly lower than the true variance in the population.
